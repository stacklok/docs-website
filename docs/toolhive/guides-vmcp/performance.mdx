---
title: Performance and sizing
description:
  Resource requirements, scaling guidance, and performance characteristics for
  Virtual MCP Server deployments.
---

This guide provides sizing recommendations and performance characteristics to
help you plan Virtual MCP Server (vMCP) deployments.

## Resource requirements

### Baseline resources

**Minimal deployment** (development/testing):

- **CPU**: 100m (0.1 cores)
- **Memory**: 128Mi

**Production deployment** (recommended):

- **CPU**: 500m (0.5 cores)
- **Memory**: 512Mi

### Scaling factors

Resource needs increase based on:

- **Number of backends**: Each backend adds minimal overhead (~10-20Mi memory)
- **Request volume**: Higher traffic requires more CPU for request processing
- **Data volume**: Large inputs and tool responses increase memory usage and
  network bandwidth
- **Composite tool complexity**: Workflows with many parallel steps consume more
  memory
- **Token caching**: Authentication token cache grows with unique client count

## Backend scale recommendations

vMCP performs well across different scales:

| Backend Count | Use Case                      | Notes                                |
| ------------- | ----------------------------- | ------------------------------------ |
| 1-5           | Small teams, focused toolsets | Minimal resource overhead            |
| 5-15          | Medium teams, diverse tools   | Recommended range for most use cases |
| 15-30         | Large teams, comprehensive    | Increase health check interval       |
| 30+           | Enterprise-scale deployments  | Consider multiple vMCP instances     |

## Scaling strategies

### Horizontal scaling

Horizontal scaling is possible for **stateless use cases** where MCP sessions
can be resumed on any vMCP instance. However, **stateful backends** (e.g.,
Playwright browser sessions, database connections) complicate horizontal scaling
because requests must be routed to the same vMCP instance that established the
session.

**Session considerations:**

- vMCP uses
  [MCP session IDs](https://spec.modelcontextprotocol.io/specification/architecture/lifecycle/#sessions)
  to cache routing tables and maintain consistency
- Some backends maintain persistent state that requires session affinity
- Clients must be able to
  [disconnect and resume sessions](https://spec.modelcontextprotocol.io/specification/architecture/lifecycle/#connection-lifecycle)
  for horizontal scaling to work reliably

**When horizontal scaling works well:**

- Stateless backends (fetch, search, read-only operations)
- Short-lived sessions with no persistent state
- Use cases where session affinity can be reliably maintained

**When horizontal scaling is challenging:**

- Stateful backends (Playwright, database connections, file system operations)
- Long-lived sessions requiring persistent state
- Complex session interdependencies

#### Configuration

To scale horizontally, increase replicas in the Deployment:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vmcp-my-vmcp
spec:
  replicas: 3 # Horizontal scaling
  # ... rest of deployment spec
```

:::caution[Backend scaling]

When scaling vMCP horizontally, the backend MCP servers will also see increased
load. Ensure your backend deployments (MCPServer resources) are also scaled
appropriately to handle the additional traffic.

:::

**Session affinity is required** when using multiple replicas. Clients must be
routed to the same vMCP instance for the duration of their session. Configure
based on your deployment:

- **Kubernetes Service**: Use `sessionAffinity: ClientIP` for basic
  client-to-pod stickiness
  - Note: This is IP-based and may not work well behind proxies or with changing
    client IPs
- **Ingress Controller**: Configure cookie-based sticky sessions (recommended)
  - nginx: Use `nginx.ingress.kubernetes.io/affinity: cookie`
  - Other controllers: Consult your Ingress controller documentation
- **Gateway API**: Use appropriate session affinity configuration based on your
  Gateway implementation

:::tip[Session affinity recommendations]

- For **stateless backends**: Cookie-based sticky sessions work well and provide
  reliable routing through proxies
- For **stateful backends** (Playwright, databases): Consider vertical scaling
  or dedicated vMCP instances instead of horizontal scaling with session
  affinity, as session resumption may not work reliably

:::

### Vertical scaling

Vertical scaling (increasing CPU/memory per instance) provides the **simplest
scaling story** and works for all use cases, including stateful backends.
However, it has limits and may not provide high availability since a single
instance failure affects all sessions.

**Recommended approach:**

- Start with vertical scaling for simplicity
- Add horizontal scaling with session affinity when vertical limits are reached
- For stateful backends, consider dedicated vMCP instances per team/use case

## When to scale

### Scale up (increase resources)

Increase CPU and memory when you observe:

- High CPU usage (>70% sustained) during normal operations
- Memory pressure or OOM (out-of-memory) kills
- Slow response times (>1 second) for simple tool calls
- Health check timeouts or frequent backend unavailability

### Scale out (increase replicas)

Add more vMCP instances when:

- CPU usage remains high despite increasing resources
- You need higher availability and fault tolerance
- Request volume exceeds capacity of a single instance
- You want to distribute load across multiple availability zones

### Scale configuration

Adjust operational settings when scaling:

<details>
<summary>Configuration for large backend counts (15+)</summary>

```yaml
spec:
  config:
    operational:
      failureHandling:
        # Reduce health check frequency to minimize overhead
        healthCheckInterval: 60s

        # Increase thresholds for better stability
        unhealthyThreshold: 5
```

</details>

<details>
<summary>Configuration for high request volumes</summary>

```yaml
spec:
  podTemplateSpec:
    spec:
      containers:
        - name: vmcp
          resources:
            requests:
              cpu: '1'
              memory: 1Gi
            limits:
              cpu: '2'
              memory: 2Gi
```

</details>

## Performance optimization

<details>
<summary>Reduce backend discovery time</summary>

1. **Use inline mode** for static backend configurations (eliminates Kubernetes
   API queries)
2. **Minimize backend count** by grouping related tools in fewer servers
3. **Optimize backend initialization** by reducing tool/resource counts or using
   lazy loading patterns in backend servers

</details>

<details>
<summary>Reduce authentication overhead</summary>

1. **Enable token caching** (enabled by default)
2. **Use unauthenticated mode** for internal/trusted backends
3. **Configure appropriate token expiration** in your OIDC provider

</details>

<details>
<summary>Optimize composite workflows</summary>

1. **Minimize dependencies** between steps to maximize parallelism
2. **Use step-level error handling** with `onError.action: continue` to prevent
   single step failures from blocking entire workflows
3. **Set appropriate timeouts** for slow backends using `timeout` field on
   individual steps

</details>

<details>
<summary>Monitor performance</summary>

Use the vMCP telemetry integration to monitor:

- Backend request latency and error rates
- Workflow execution times and failure patterns
- Health check success/failure rates

See [Telemetry and metrics](./telemetry-and-metrics.mdx) for configuration
details.

</details>

## Related information

- [Introduction to vMCP](./intro.mdx)
- [Configure health checks](./configuration.mdx#health-checks)
- [Backend discovery modes](./backend-discovery.mdx)
- [Telemetry and metrics](./telemetry-and-metrics.mdx)
- [VirtualMCPServer CRD specification](../reference/crd-spec.md#apiv1alpha1virtualmcpserver)
